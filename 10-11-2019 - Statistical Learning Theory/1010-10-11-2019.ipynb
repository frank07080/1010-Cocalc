{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning Theory\n",
    "\n",
    "#### *11 October 2019*\n",
    "#### *DATA 1010*\n",
    "\n",
    "The goal of **statistical learning** is to draw conclusions about an unknown probability measure given independent observations drawn from the measure. These observations are called **training data**. In **supervised learning**, the unknown measure $\\mathbb{P}$ is on a product space $\\mathcal{X} \\times \\mathcal{Y}$. In other words, each training observation has the form $(\\mathbf{X}, Y)$ where $\\mathbf{X}$ is an element of $\\mathcal{X}$ and $\\mathbf{Y}$ is an element of $\\mathcal{Y}$. \n",
    "\n",
    "We aim to use the training data to predict $Y$ given $\\mathbf{X}$, where $(\\mathbf{X},Y)$ denotes a random variable in $\\mathcal{X} \\times \\mathcal{Y}$ with distribution $\\mathbb{P}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Suppose that $\\mathbf{X} = [X_1, X_2]$, where $X_1$ is the color of a banana, $X_2$ is the weight of the banana, and $Y$ is a measure of banana deliciousness. Values of $X_1, X_2,$ and $Y$ are recorded for many bananas, and they are used to predict $Y$ for other bananas whose $\\mathbf{X}$ values are known. \n",
    "\n",
    "Do you expect the prediction function to be more sensitive to changes in $X_1$ or changes in $X_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We call the components of $\\mathbf{X}$ *features*, *predictors*, or *input variables*, and we call $Y$ the *response variable* or *output variable*.\n",
    "\n",
    "A supervised learning problem is a **regression** problem if $Y$ is quantitative ($\\mathcal{Y}\\subset \\mathbb{R}$) and a **classification** problem if $\\mathcal{Y}$ is a set of labels. \n",
    "\n",
    "To make meaningful and unambiguous statements about a proposed prediction function $h: \\mathcal{X} \\to \\mathcal{Y}$, we need a rubric by which to assess it. This is customarily done by defining a *loss* (or *risk*, or *error*) $L(h)$, with the idea that smaller loss is better. We might wish to define $L$ only for $h$'s in a specified class $\\mathcal{H}$ of candidate functions. Since $L : \\mathcal{H} \\to \\mathbb{R}$ is defined on a set of functions, we call $L$ the **loss functional**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a statistical learning problem, a space $\\mathcal{H}$ of candidate prediction functions, and a loss functional $L: \\mathcal{H} \\to \\mathbb{R}$, we define the **target function** to be $\\operatorname{argmin}_{h \\in \\mathcal{H}}L(h)$. \n",
    "\n",
    "Let's look at some common loss functionals. For regression, we often use the **mean squared error**: \n",
    "\n",
    "$$\n",
    "L(h) = \\mathbb{E}[(h(X)-Y)^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Think of a loss functional for a regression problem aside from the mean squared error. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we consider the **misclassification probability** \n",
    "\n",
    "$$\n",
    "L(h) = \\mathbb{E}\\left[\\boldsymbol{1}_{\\{h(\\mathbf{X}) \\neq Y\\}}\\right] = \\mathbb{P}(h(\\mathbf{X}) \\neq Y).  \n",
    "$$\n",
    "\n",
    "If $\\mathcal{H}$ contains $G(\\mathbf{x}) = \\operatorname{argmax}_c\\mathbb{P}(Y=c | \\mathbf{X} = \\mathbf{x})$, then $G$ is the target function for this loss functional. \n",
    "\n",
    "Note that neither of these loss functionals can be computed directly unless the probability measure $\\mathbb{P}$ on $\\mathcal{X} \\times \\mathcal{Y}$ is known. Since the goal of statistical learning is to make inferences about $\\mathbb{P}$ when it is *not* known, we must approximate $L$ (and likewise also the target function $h$) using the training data. \n",
    "\n",
    "The most straightforward way to do this is to replace $\\mathbb{P}$ with the **empirical probability measure** associated with the training data $\\{(\\mathbf{X}_i, Y_i)\\}_{i=1}^n$. This is the probability measure which places $\\frac{1}{n}$ units of probability mass at $(\\mathbf{X}_i, Y_i)$, for each $i$ from $1$ to $n$. The **empirical risk** of a candidate function $h \\in \\mathcal{H}$ is the risk functional evaluated with respect to the empirical measure of the training data. \n",
    "\n",
    "A **learner** is a function which takes a set of training data as input and returns a prediction function $\\widehat{h}$ as output. A common way to specify a learner is to let $\\widehat{h}$ be the **empirical risk minimizer** (ERM), which is the function in $\\mathcal{H}$ which minimizes the empirical risk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Suppose that $\\mathcal{X} = [0,1]$ and $\\mathcal{Y} = \\mathbb{R}$, and that the probability measure on $\\mathcal{X} \\times \\mathcal{Y}$ is the one which corresponds to sampling $X$ uniformly from $[0,1]$ and then sampling $Y$ from $\\mathcal{N}(X/2 + 1, 1)$. \n",
    "\n",
    "Let $\\mathcal{H}$ be the set of monic polynomials of degree six or less. Given training observations $\\{(\\mathbf{X}_i, Y_i)\\}_{i=1}^6$, find the risk minimizer and the empirical risk minimizer for the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `/opt/julia/environments/v1.2/Project.toml`\n",
      " \u001b[90m [f27b6e38]\u001b[39m\u001b[92m + Polynomials v0.5.2\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `/opt/julia/environments/v1.2/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5300\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5300)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5301\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5300)\" points=\"\n",
       "113.754,1487.47 2352.76,1487.47 2352.76,47.2441 113.754,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5302\">\n",
       "    <rect x=\"113\" y=\"47\" width=\"2240\" height=\"1441\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  177.122,1487.47 177.122,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  705.189,1487.47 705.189,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1233.26,1487.47 1233.26,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1761.32,1487.47 1761.32,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2289.39,1487.47 2289.39,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  113.754,1487.47 2352.76,1487.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  113.754,1007.4 2352.76,1007.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  113.754,527.321 2352.76,527.321 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  113.754,47.2441 2352.76,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  113.754,1487.47 2352.76,1487.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  113.754,1487.47 113.754,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  177.122,1487.47 177.122,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  705.189,1487.47 705.189,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1233.26,1487.47 1233.26,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1761.32,1487.47 1761.32,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2289.39,1487.47 2289.39,1465.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  113.754,1487.47 147.339,1487.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  113.754,1007.4 147.339,1007.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  113.754,527.321 147.339,527.321 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  113.754,47.2441 147.339,47.2441 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 177.122, 1541.47)\" x=\"177.122\" y=\"1541.47\">0.00</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 705.189, 1541.47)\" x=\"705.189\" y=\"1541.47\">0.25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1233.26, 1541.47)\" x=\"1233.26\" y=\"1541.47\">0.50</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1761.32, 1541.47)\" x=\"1761.32\" y=\"1541.47\">0.75</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2289.39, 1541.47)\" x=\"2289.39\" y=\"1541.47\">1.00</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 89.7545, 1504.97)\" x=\"89.7545\" y=\"1504.97\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 89.7545, 1024.9)\" x=\"89.7545\" y=\"1024.9\">1</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 89.7545, 544.821)\" x=\"89.7545\" y=\"544.821\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 89.7545, 64.7441)\" x=\"89.7545\" y=\"64.7441\">3</text>\n",
       "</g>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"1800.29\" cy=\"352.001\" r=\"18\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"1800.29\" cy=\"352.001\" r=\"14\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"2163.74\" cy=\"817.876\" r=\"18\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"2163.74\" cy=\"817.876\" r=\"14\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"1600.7\" cy=\"714.169\" r=\"18\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"1600.7\" cy=\"714.169\" r=\"14\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"1012.42\" cy=\"1005.72\" r=\"18\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"1012.42\" cy=\"1005.72\" r=\"14\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"838.777\" cy=\"1095.13\" r=\"18\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"838.777\" cy=\"1095.13\" r=\"14\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"1576.61\" cy=\"1253.49\" r=\"18\"/>\n",
       "<circle clip-path=\"url(#clip5302)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"1576.61\" cy=\"1253.49\" r=\"14\"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  177.122,967048 198.245,878059 219.368,795460 240.49,718918 261.613,648108 282.736,582719 303.858,522449 324.981,467008 346.104,416116 367.226,369504 \n",
       "  388.349,326913 409.472,288093 430.594,252806 451.717,220822 472.84,191922 493.962,165895 515.085,142540 536.208,121664 557.33,103083 578.453,86622.1 \n",
       "  599.576,72115 620.698,59402.8 641.821,48334.8 662.943,38768.2 684.066,30567.8 705.189,23605.7 726.311,17761.5 747.434,12921.4 768.557,8978.92 789.679,5833.9 \n",
       "  810.802,3392.78 831.925,1568.24 853.047,279.056 874.17,-550.098 895.293,-988.853 916.415,-1101.32 937.538,-946.298 958.661,-577.439 979.783,-43.4582 1000.91,611.68 \n",
       "  1022.03,1348.58 1043.15,2132.2 1064.27,2931.72 1085.4,3720.26 1106.52,4474.78 1127.64,5175.83 1148.76,5807.4 1169.89,6356.7 1191.01,6813.97 1212.13,7172.33 \n",
       "  1233.26,7427.52 1254.38,7577.79 1275.5,7623.64 1296.62,7567.67 1317.75,7414.39 1338.87,7170 1359.99,6842.23 1381.11,6440.14 1402.24,5973.92 1423.36,5454.72 \n",
       "  1444.48,4894.43 1465.6,4305.52 1486.73,3700.84 1507.85,3093.43 1528.97,2496.3 1550.1,1922.3 1571.22,1383.89 1592.34,892.929 1613.46,460.547 1634.59,96.9005 \n",
       "  1655.71,-188.993 1676.83,-389.452 1697.95,-498.319 1719.08,-511.152 1740.2,-425.414 1761.32,-240.665 1782.44,41.2516 1803.57,416.015 1824.69,876.637 1845.81,1413.28 \n",
       "  1866.93,2013.04 1888.06,2659.81 1909.18,3334.02 1930.3,4012.5 1951.43,4668.28 1972.55,5270.37 1993.67,5783.62 2014.79,6168.48 2035.92,6380.83 2057.04,6371.81 \n",
       "  2078.16,6087.6 2099.28,5469.23 2120.41,4452.43 2141.53,2967.38 2162.65,938.564 2183.77,-1715.44 2204.9,-5082.13 2226.02,-9255.3 2247.14,-14335.2 2268.27,-20428.8 \n",
       "  2289.39,-27649.8 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5302)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  177.122,1007.4 198.245,1005 219.368,1002.6 240.49,1000.2 261.613,997.796 282.736,995.396 303.858,992.996 324.981,990.595 346.104,988.195 367.226,985.794 \n",
       "  388.349,983.394 409.472,980.994 430.594,978.593 451.717,976.193 472.84,973.792 493.962,971.392 515.085,968.992 536.208,966.591 557.33,964.191 578.453,961.791 \n",
       "  599.576,959.39 620.698,956.99 641.821,954.589 662.943,952.189 684.066,949.789 705.189,947.388 726.311,944.988 747.434,942.587 768.557,940.187 789.679,937.787 \n",
       "  810.802,935.386 831.925,932.986 853.047,930.586 874.17,928.185 895.293,925.785 916.415,923.384 937.538,920.984 958.661,918.584 979.783,916.183 1000.91,913.783 \n",
       "  1022.03,911.382 1043.15,908.982 1064.27,906.582 1085.4,904.181 1106.52,901.781 1127.64,899.381 1148.76,896.98 1169.89,894.58 1191.01,892.179 1212.13,889.779 \n",
       "  1233.26,887.379 1254.38,884.978 1275.5,882.578 1296.62,880.177 1317.75,877.777 1338.87,875.377 1359.99,872.976 1381.11,870.576 1402.24,868.176 1423.36,865.775 \n",
       "  1444.48,863.375 1465.6,860.974 1486.73,858.574 1507.85,856.174 1528.97,853.773 1550.1,851.373 1571.22,848.972 1592.34,846.572 1613.46,844.172 1634.59,841.771 \n",
       "  1655.71,839.371 1676.83,836.971 1697.95,834.57 1719.08,832.17 1740.2,829.769 1761.32,827.369 1782.44,824.969 1803.57,822.568 1824.69,820.168 1845.81,817.767 \n",
       "  1866.93,815.367 1888.06,812.967 1909.18,810.566 1930.3,808.166 1951.43,805.766 1972.55,803.365 1993.67,800.965 2014.79,798.564 2035.92,796.164 2057.04,793.764 \n",
       "  2078.16,791.363 2099.28,788.963 2120.41,786.562 2141.53,784.162 2162.65,781.762 2183.77,779.361 2204.9,776.961 2226.02,774.561 2247.14,772.16 2268.27,769.76 \n",
       "  2289.39,767.359 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip5300)\" points=\"\n",
       "1540.74,372.684 2280.76,372.684 2280.76,130.764 1540.74,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1540.74,372.684 2280.76,372.684 2280.76,130.764 1540.74,130.764 1540.74,372.684 \n",
       "  \"/>\n",
       "<circle clip-path=\"url(#clip5300)\" style=\"fill:#000000; stroke:none; fill-opacity:1\" cx=\"1648.74\" cy=\"191.244\" r=\"25\"/>\n",
       "<circle clip-path=\"url(#clip5300)\" style=\"fill:#009af9; stroke:none; fill-opacity:1\" cx=\"1648.74\" cy=\"191.244\" r=\"21\"/>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1732.74, 208.744)\" x=\"1732.74\" y=\"208.744\">training points</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1564.74,251.724 1708.74,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1732.74, 269.224)\" x=\"1732.74\" y=\"269.224\">empirical risk minimizer</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5300)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1564.74,312.204 1708.74,312.204 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5300)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 1732.74, 329.704)\" x=\"1732.74\" y=\"329.704\">risk minimizer</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"Polynomials\")\n",
    "using Plots, Polynomials, Random\n",
    "Random.seed!(123)\n",
    "X = rand(6)\n",
    "Y = X/2 .+ 1 .+ randn(6)\n",
    "p = polyfit(X,Y)\n",
    "scatter(X, Y, label = \"training points\", ylims = (0,3))\n",
    "plot!(0:0.01:1, x->p(x), label = \"empirical risk minimizer\")\n",
    "plot!(0:0.01:1, x->x/2+1, label = \"risk minimizer\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates a phenomenon called **overfitting**. Although the empirical risk is small for the prediction function $h$ we found, smallness of the empirical risk does not imply smallness of the true risk. The difference between empirical risk and the actual value of the risk functional is called **generalization error** (or *test* error). \n",
    "\n",
    "We mitigate overfitting by building [**inductive bias**](gloss:inductive-bias) into the model. Common approaches include \n",
    "* using a restrictive class $\\mathcal{H}$ of candidate functions, \n",
    "* **regularizing**: adding a term to the loss functional which penalizes model complexity, and \n",
    "* **cross-validating**: proposing a spectrum of candidate functions and selecting the one which performs best on withheld training observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "* Which method of introducing inductive bias does linear regression use? \n",
    "* Which method did we use for kernel density estimation in the [statistics course](https://mathigon.org/course/intro-statistics/estimating-joint-densities)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inductive bias can lead to **underfitting**: relevant relations are missed, so both training and test error are larger than necessary. The tension between underfitting and overfitting is the **bias-complexity** (or *bias-variance*) **tradeoff**. \n",
    "\n",
    "**Theorem** (no free lunch)  \n",
    "Suppose $\\mathcal{X}$ and $\\mathcal{Y}$ are finite sets, and let $f$ denote a probability distribution on $\\mathcal{X} \\times \\mathcal{Y}$. Let $D$ be a collection of $n$ independent observations from $f$, and let $h_1$ and $h_2$ be prediction functions (which associate a prediction $h_j(d,\\mathbf{x}) \\in \\mathcal{Y}$ to each pair $(d,\\mathbf{x})$ where $d$ is a set of training observations and $\\mathbf{x} \\in \\mathcal{X}$). Consider the cost random variable $C_j = (h_j(D,X) - Y)^2$ (or $C_j = \\boldsymbol{1}_{\\{h_j(D,X) - Y\\}}$) for $j \\in \\{1,2\\}$. \n",
    "  \n",
    "The average over all distributions $f$ of $C_1$ is equal to the average over all distributions $f$ of $C_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "The no-free-lunch theorem implies that cross-validation is exactly as effective on average as anti-cross-validation (select the worst-performing model on withheld data). What does that imply about the relationship between real-world probability distributions and the space of all probability distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "To estimate $\\boldsymbol{\\beta} = [\\beta_0,\\beta_1]$ from the $n$ observations $\\{(x_i,y_i)\\}_{i=1}^n$, we minimize the empirical mean squared error (also known as the **residual sum of squares**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\operatorname{RSS}(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2.   \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Find the value of $\\boldsymbol{\\beta}$ which minimizes $\\operatorname{RSS}(\\boldsymbol{\\beta})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 \n",
    "\n",
    "Show how to use linear regression to find the polynomial of degree $d$ which minimizes the residual sum of squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "In this section, we'll take a look at a classification problem. \n",
    "\n",
    "<img src=\"flower-points.svg\" style=\"float: right;\">\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "Given a flower randomly selected from a field, let $X_1$ be its petal width in centimeters, $X_2$ its petal length in centimeters, and $Y\\in\\{\\mathrm{R},\\mathrm{G},\\mathrm{B}\\}$ its color. Let\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\mu}_{\\mathrm{R}} &=\n",
    "\\left[\\begin{smallmatrix} 9 \\\\ 5 \\end{smallmatrix}\\right]\n",
    "  &\\quad \\boldsymbol{\\mu}_{\\mathrm{G}} &=\n",
    "  \\left[\\begin{smallmatrix}4 \\\\ 10 \\end{smallmatrix}\\right]\n",
    "    &\\quad \\boldsymbol{\\mu}_{\\mathrm{B}} &= \\left[\\begin{smallmatrix}7 \\\\ 9 \\end{smallmatrix}\\right] \\\\\n",
    "A_{\\mathrm{R}} &= \\left[\\begin{smallmatrix}1.5 & -1 \\\\ 0 & \\hphantom{-}1\n",
    "              \\\\ \\end{smallmatrix}\\right] &\\quad \n",
    "A_{\\mathrm{G}} &= \\left[\\begin{smallmatrix}0.5 & 0.25 \\\\ 0 & 0.5\n",
    "              \\\\ \\end{smallmatrix}\\right] &\\quad \n",
    "A_{\\mathrm{B}} &= \\left[\\begin{smallmatrix}2 & 0 \\\\ 0 & 2\n",
    "              \\\\ \\end{smallmatrix}\\right]. \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Suppose that the joint distribution of $X_1, X_2,$ and $Y$ is defined as follows: for any $A\\subset \\mathbb{R}^2$ and color $c \\in \\{\\mathrm{R},\\mathrm{G},\\mathrm{B}\\}$, we have\n",
    "\n",
    "$$\n",
    "  \\mathbb{P}(A \\times \\{c\\}) = p_c\\int_{\\mathbb{R}^2} f_c(x_1,x_2) \\operatorname{d} x_1 \n",
    "  \\operatorname{d} x_2, \n",
    "$$\n",
    "\n",
    "where $(p_R,p_G,p_B) = (1/3,1/6,1/2)$ and $f_c$ is the multivariate normal density with mean $\\mu_c$ and covariance matrix $A_cA_c'$. In other words, we can sample from the joint distribution of of $X_1, X_2,$ and $Y$ by sampling $Y$ from {R, G, B} with probabilities 1/3, 1/6, and 1/2, respectively, and then generate $(X_1, X_2)$ by calculating $A_Y Z + \\mu_Y$, where $Z$ is a vector of two standard normal random variables which are independent and independent of $Y$. \n",
    "\n",
    "Three hundred observations from the distribution of $(X_1, X_2, Y)$ are\n",
    "shown in the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best predictor of $Y$ given $(X_1,X_2) = (x_1, x_2)$ (using the 0-1 loss function), and find a way to estimate that predictor using the given observations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Distributions, Random; Random.seed!(1234)\n",
    "struct Flower\n",
    "    X::Vector\n",
    "    color::String\n",
    "end\n",
    "# density function for the normal distribution N\n",
    "xs = 0:1/2^4:15\n",
    "ys = 0:1/2^4:15\n",
    "As = [[1.5 -1; 0 1],[1/2 1/4; 0 1/2], [2 0; 0 2]]\n",
    "μs = [[9,5],[4,10],[7,9]]\n",
    "Ns = [MvNormal(μ,A*A') for (μ,A) in zip(μs,As)]\n",
    "p = Weights([1/3,1/6,1/2])\n",
    "colors = [\"red\",\"green\",\"blue\"]\n",
    "function randflower(μs,As)\n",
    "    i = sample(p)\n",
    "    Flower(As[i]*randn(2)+μs[i],colors[i])\n",
    "end\n",
    "flowers = [randflower(μs,As) for i=1:300]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
